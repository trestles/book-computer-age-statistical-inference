## 3 Bayesian Inference
- The human mind is an inference machine: "It's getting windy, the sky is darkening, I'd better bring my umbrella with me." Unfortunately, it's not a very dependable machine especially when weighing complicated choices against past experiences. p22
- normal family p22
- Poisson family p23
- Exactly what constitutes "prior knowledge" is a crucial question we will consider in ongoing discussions of Bayes' theorem. p23
### 3.1 Two Examples p24
- Physicist's twins example p24
- The prior for the twins problem was based on a large amount of relevant previous experience. Such experience is most often unavailable. Modern Bayesian practice uses various strategies to construct an appropriate "prior" g(mu) in the absence of prior experience, leaving many statisticians unconvinced by the resulting Bayesian inferences. Our second example illustrates this difficulty. p26
- One expedient, going back to Laplace, is the "principle of insufficient reason", that is we take theta to be uniformly distributed over chi  a flat prior p27
- Its effect is seen in the leftward shift of the posterior density. Shrinkage priors will play a major role in our discussion of large scale estimation and testing problems, where we are hoping to find a few large effects hidden among thousands of negligible ones. p28
### 3.2 Uninformative Prior Distributions p28
### 3.3 Flaws in Frequentist Inference p30
### 3.4 A Bayesian/Frequentist Comparison List p33
- Bayes' rule was used to devastating effect before the 2012 US presidential election, updating sequential polling results to correctly predict the outcome in all 50 states. Bayes' theorem is an excellent tool in general for combining statistical evidence from disparate sources, the closest frequentist analog being maximum likelihood estimation. p35
### 3.5 Notes and Details p36
